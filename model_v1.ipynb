{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# !pip install opencv-python-headless\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam, legacy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cfebdb4e54530dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### load in all the CSV's for each ICH\n",
    "\n",
    "## THIS IS THE CORRECT WAY THAT WE NEED TO TRY, NOT LOADING FROM IMG DIRECTORY\n",
    "\n",
    "EPH_df = pd.read_csv('Hemorrhage Segmentation Project/Results_Epidural Hemorrhage Detection_2020-11-16_21.31.26.148.csv') \n",
    "EPH_df = EPH_df[['Origin', 'Correct Label']]\n",
    "\n",
    "IVH_df = pd.read_csv('Hemorrhage Segmentation Project/Results_Brain Hemorrhage Tracing_2020-09-28_15.21.52.597.csv') # IVH\n",
    "IVH_df = IVH_df[['Origin', 'Correct Label']]\n",
    "\n",
    "IPH_df = pd.read_csv('Hemorrhage Segmentation Project/Results_Intraparenchymal Hemorrhage Detection_2020-11-16_21.39.31.268.csv')\n",
    "IPH_df = IPH_df[['Origin', 'Correct Label']]\n",
    "\n",
    "SDH_df = pd.read_csv('Hemorrhage Segmentation Project/Results_Subdural Hemorrhage Detection_2020-11-16_21.37.19.745.csv')\n",
    "SDH_df = SDH_df[['Origin', 'Correct Label']]\n",
    "\n",
    "SAH_df = pd.read_csv('Hemorrhage Segmentation Project/Results_Subarachnoid Hemorrhage Detection_2020-11-16_21.36.18.668.csv')\n",
    "SAH_df = SAH_df[['Origin', 'Correct Label']]\n",
    "\n",
    "MCH_df = pd.read_csv('Hemorrhage Segmentation Project/Results_Multiple Hemorrhage Detection_2020-11-16_21.36.24.018.csv') # MCH\n",
    "MCH_df = MCH_df[['Origin', 'Correct Label']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c779ab94e48e672"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPH_filter = EPH_df['Correct Label'].notna()\n",
    "EPH_filtered = EPH_df[EPH_filter]\n",
    "len(EPH_filtered)\n",
    "EPH_filtered"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "832aaea18414c598"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SDH_df['Labeling State'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac25d84a2b41e8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SDH_filter = SDH_df['Labeling State'] != 'In Progress'\n",
    "SDH_filtered = SDH_df[EPH_filter]\n",
    "len(SDH_filtered)\n",
    "SDH_filtered"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe46cbc480e1ed27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define your image directories\n",
    "file_dir = 'XN1 Data/renders/' \n",
    "ICH_types = ['normal', 'epidural', 'subarachnoid', 'intraparenchymal', 'subdural', 'intraventricular', 'multi']\n",
    "windows = [\"brain_bone_window\", \"brain_window\", \"max_contrast_window\", \"subdural_window\"]\n",
    "\n",
    "# Read flagged filenames from flagged.txt\n",
    "with open('Hemorrhage Segmentation Project/flagged.txt', 'r') as f:\n",
    "    flagged_files = set(line.strip() for line in f)  # Using a set for fast membership checking\n",
    "\n",
    "#normal file_max is 6080 (since not same number of files in all windows)\n",
    "\n",
    "# One Hot Encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit([[0],[1],[2],[3],[4],[5],[6]])\n",
    "target_shape = (512, 512, 3)\n",
    "\n",
    "def load_images_from_directory(directory, label, target_shape, file_max=None):\n",
    "    \"\"\"Load images from a given directory, check shape, and apply one-hot encoding.\"\"\"\n",
    "\n",
    "    train_data = []\n",
    "    # labels = []\n",
    "    # wrong_files = []\n",
    "    same_scan = set()\n",
    "    \n",
    "    # for dirname, _, filenames in os.walk(directory):\n",
    "    filenames = os.listdir(directory)\n",
    "    if file_max is None:\n",
    "        file_max = len(filenames)\n",
    "        \n",
    "    true_file_max = file_max\n",
    "        \n",
    "    for filename in tqdm(filenames[0:file_max], desc=f'Loading images from {directory}', unit='file'):\n",
    "        if filename in flagged_files:\n",
    "            true_file_max -= 1\n",
    "            continue\n",
    "        else:\n",
    "            img_path = os.path.join(directory, filename)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Convert to array and check shape\n",
    "            img_np = np.array(img)\n",
    "            if img_np.shape != target_shape:\n",
    "                # wrong_files.append(filename) \n",
    "                img = img.resize((512,512), Image.LANCZOS)  # Use LANCZOS for high-quality downsampling\n",
    "               \n",
    "            img = img.resize((256,256), Image.LANCZOS)  # Use LANCZOS for high-quality downsampling\n",
    "            \n",
    "            img_array = np.array(img)\n",
    "            grayscale_img = img_array\n",
    "            # grayscale_img = np.mean(img_array, axis=2)\n",
    "            grayscale_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            train_data.append(grayscale_img)\n",
    "            \n",
    "            # if filename not in same_scan:\n",
    "            #     labels.append(encoder.transform([[label]]).toarray()[0])\n",
    "            \n",
    "            same_scan.add(filename)\n",
    "                \n",
    "    return train_data, file_max\n",
    "\n",
    "\n",
    "def main_loader(directory, windows, label, X_train_dict, y_train, target_shape, file_max = None):\n",
    "    # Assuming ICH_types is defined somewhere above\n",
    "    for i, slide in enumerate(windows):\n",
    "        dir_path = os.path.join(directory, ICH_types[label], slide)  # Adjust path based on your directory structure\n",
    "        \n",
    "        # Use tqdm to show progress for the main loader\n",
    "        print(f\"Loading images for {slide}...\")\n",
    "        \n",
    "        data, file_max = load_images_from_directory(dir_path, label, target_shape, file_max)  # Load images\n",
    "        X_train_dict[slide].extend(data)  # Append data to the corresponding window\n",
    "\n",
    "    ## THIS TAKES A LONG TIME FOR BIG ARRAYS\n",
    "    print(\"Converting lists to numpy arrays:\")\n",
    "    for window in tqdm(windows, desc='Converting lists', unit='window'):\n",
    "        X_train_dict[window] = np.array(X_train_dict[window])\n",
    "\n",
    "    sample = encoder.transform([[label]]).toarray()\n",
    "    labels = np.tile(sample, (file_max, 1))\n",
    "    y_train = np.array(labels)\n",
    "    \n",
    "    return X_train_dict, y_train\n",
    "\n",
    "def stack_slices(X_train_dict):\n",
    "    # Get the windows to stack in the order you want\n",
    "    windows_to_stack = [\n",
    "        'brain_bone_window', \n",
    "        'brain_window', \n",
    "        'subdural_window', \n",
    "        'max_contrast_window'\n",
    "    ]\n",
    "    \n",
    "    # Create an empty list to hold the stacked data\n",
    "    stacked_data = []\n",
    "\n",
    "    # Use tqdm to show progress for stacking\n",
    "    for window in tqdm(windows_to_stack, desc='Stacking slices', unit='window'):\n",
    "        stacked_data.append(X_train_dict[window])\n",
    "\n",
    "    # Stack the windows along a new axis\n",
    "    X_train = np.stack(stacked_data, axis=3)\n",
    "    \n",
    "    return X_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ecab90bf2824046"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Input Struct\n",
    "# Shape will be (N, 512, 512, 4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603372543107c211"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load images for No Hemorrhage (Normal)\n",
    "\n",
    "NRM_train_dict = {window: [] for window in windows}  # Create a dictionary to hold data for each window\n",
    "NRM_label = []  # This will hold the labels\n",
    "\n",
    "NRM_train_dict, NRM_label = main_loader(file_dir, windows, 0, NRM_train_dict, NRM_label, target_shape, 2000)\n",
    "\n",
    "NRM_train = stack_slices(NRM_train_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb75b92430de4d29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(NRM_train[0][:,:,0], cmap='binary')\n",
    "\n",
    "# NRM_train[0][:, :, 0].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1edbe4f5c146f3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(NRM_train[0][:,:,1], cmap='binary')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ebda21d58776854"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load images for Epidural Hemorrhage\n",
    "\n",
    "EDH_train_dict = {window: [] for window in windows}  # Create a dictionary to hold data for each window\n",
    "EDH_label = []  # This will hold the labels\n",
    "\n",
    "\n",
    "EDH_train_dict, EDH_label = main_loader(file_dir, windows, 1, EDH_train_dict, EDH_label, target_shape)\n",
    "EDH_train = stack_slices(EDH_train_dict)\n",
    "\n",
    "sample = encoder.transform([[1]]).toarray()\n",
    "labels = np.tile(sample, (len(EDH_train), 1))\n",
    "EDH_label = np.array(labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a71b07e069ae7a7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(NRM_train[0], cmap='binary')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8e2e4740394f44b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load images for Subarachnoid Hemorrhage\n",
    "\n",
    "SAH_train_dict = {window: [] for window in windows}  # Create a dictionary to hold data for each window\n",
    "SAH_label = []  # This will hold the labels\n",
    "\n",
    "SAH_train_dict, SAH_label = main_loader(file_dir, windows, 2, SAH_train_dict, SAH_label, target_shape, 2000)\n",
    "\n",
    "SAH_train = stack_slices(SAH_train_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd1f262c9b1843d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(SAH_train[0], cmap='binary')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ff1f318502ec499"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load images for Intraparenchymal Hemorrhage\n",
    "\n",
    "IPH_train_dict = {window: [] for window in windows}  # Create a dictionary to hold data for each window\n",
    "IPH_label = []  # This will hold the labels\n",
    "\n",
    "IPH_train_dict, IPH_label = main_loader(file_dir, windows, 3, IPH_train_dict, IPH_label, target_shape, 2000)\n",
    "\n",
    "\n",
    "IPH_train = stack_slices(IPH_train_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1875a210346efd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load images for Subdural Hemorrhage\n",
    "\n",
    "SDH_train_dict = {window: [] for window in windows}  # Create a dictionary to hold data for each window\n",
    "SDH_label = []  # This will hold the labels\n",
    "\n",
    "SDH_train_dict, SDH_label = main_loader(file_dir, windows, 4, SDH_train_dict, SDH_label, target_shape, 2000)\n",
    "\n",
    "\n",
    "SDH_train = stack_slices(SDH_train_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dde89a6768cb91d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load images for Intraventricular Hemorrhage\n",
    "\n",
    "IVH_train_dict = {window: [] for window in windows}  # Create a dictionary to hold data for each window\n",
    "IVH_label = []  # This will hold the labels\n",
    "\n",
    "IVH_train_dict, IVH_label = main_loader(file_dir, windows, 5, IVH_train_dict, IVH_label, target_shape, 2000)\n",
    "\n",
    "IVH_train = stack_slices(IVH_train_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dbe8ace8a0ef3bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load images for Multiclass Hemorrhage\n",
    "\n",
    "MCH_train_dict = {window: [] for window in windows}  # Create a dictionary to hold data for each window\n",
    "MCH_label = []  # This will hold the labels\n",
    "\n",
    "MCH_train_dict, MCH_label = main_loader(file_dir, windows, 6, MCH_train_dict, MCH_label, target_shape, 2000)\n",
    "\n",
    "MCH_train = stack_slices(MCH_train_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1668bf59d5c70982"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"normal\": NRM_train,\n",
    "    \"epidural\": EDH_train,\n",
    "    \"subarachnoid\": SAH_train,\n",
    "    \"intraparenchymal\": IPH_train,\n",
    "    \"subdural\": SDH_train,\n",
    "    \"intraventricular\": IVH_train,\n",
    "    \"multi\": MCH_train\n",
    "}\n",
    "\n",
    "print(EDH_train.shape)\n",
    "print(EDH_label.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8bb089fe6213f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = np.concatenate((NRM_train, EDH_train, SAH_train, IPH_train, SDH_train, IVH_train, MCH_train), axis=0)\n",
    "print(X_train.shape)\n",
    "y_train = np.concatenate((NRM_label, EDH_label, SAH_label, IPH_label, SDH_label, IVH_label, MCH_label), axis=0)\n",
    "print(y_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b6210f7b35b1df5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_valid, y_valid = shuffle(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e82db941c3cef6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7b06546852a051e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# First Conv3D layer with 32 filters and a 3x3x3 kernel\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3), input_shape=(256, 256, 4), padding=\"same\"))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))  # Pooling only in spatial dimensions\n",
    "\n",
    "# Second Conv3D layer with 64 filters\n",
    "model.add(layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\"))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third Conv3D layer with 128 filters\n",
    "model.add(layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\"))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Fourth Conv3D layer with 256 filters for deeper feature extraction\n",
    "model.add(layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\"))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))  # Pooling only in spatial dimensions\n",
    "\n",
    "# Flatten and fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dense(512))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "# Output layer with softmax activation for 7 classes\n",
    "model.add(layers.Dense(7, activation=\"softmax\"))\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95492db3abc16344"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=legacy.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c53be96cfda110"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
    "# historysgd = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), class_weight=class_weight_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67e7c4faa51b2d4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "81a7796c4a523ee9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
